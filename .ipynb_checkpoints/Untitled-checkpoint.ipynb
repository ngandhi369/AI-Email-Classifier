{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#Not using stemming as the performance improvement wasn't observed.\n",
    "#from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11360</th>\n",
       "      <td>11 21 00 discrepancies deal 464181 diana schol...</td>\n",
       "      <td>deal discrepancies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11438</th>\n",
       "      <td>deal 431849 01 kate aquila noted confirmation ...</td>\n",
       "      <td>deal discrepancies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>enron cover letter resume dave gershenson vinc...</td>\n",
       "      <td>resumes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Subject            Category\n",
       "11360  11 21 00 discrepancies deal 464181 diana schol...  deal discrepancies\n",
       "11438  deal 431849 01 kate aquila noted confirmation ...  deal discrepancies\n",
       "5352   enron cover letter resume dave gershenson vinc...             resumes"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = pd.read_csv('preprocessed1.csv')\n",
    "em = emails.dropna(axis=0)\n",
    "em.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logistics              1170\n",
       "tw-commercial group    1150\n",
       "bill williams iii      1004\n",
       "california              982\n",
       "deal discrepancies      878\n",
       "management              799\n",
       "calendar                700\n",
       "esvl                    663\n",
       "tufco                   604\n",
       "resumes                 599\n",
       "e-mail bin              592\n",
       "ces                     572\n",
       "online trading          567\n",
       "junk                    544\n",
       "junk file               494\n",
       "ooc                     473\n",
       "genco-jv_ipo            465\n",
       "projects                459\n",
       "corporate               420\n",
       "archives                419\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(textArray):\n",
    "    #If using stemming...\n",
    "    #stemmer = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "    processed_text = []\n",
    "    for text in textArray:\n",
    "        words_list = (str(text).lower()).split()\n",
    "        final_words = [wnl.lemmatize(word) for word in words_list if word not in stopwords.words('english')]\n",
    "        #If using stemming...\n",
    "        #final_words = [stemmer.stem(word) for word in words_list if word not in stopwords.words('english')]\n",
    "        final_words_str = str((\" \".join(final_words)))\n",
    "        processed_text.append(final_words_str)\n",
    "    return processed_text\n",
    "\n",
    "em['Subject'] = pre_process_text(em['Subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ 'logistics','tw-commercial group','california','bill williams iii','deal discrepancies','management','calender','esvl','tufco','resumes','e-mail bin','ces','online trading','junk','junk file','ooc','genco','projects','corporate','archives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every additional parameter value here will increase the training time by orders of magnitude. \n",
    "# I'm running on a relatively slow computer, hence reduced the values\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.0),#0.6, 0.7, 0.8, 0.9, 1.0),\n",
    "    'vect__max_features': (None, 1000, 5000),#2000, 3000, 4000, 5000, 6000, 10000, 20000, 30000, 40000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),#, (1, 3)),  # unigrams or bigrams or trigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.1, 0.01, 0.001),#, 0.0001, 0.00001, 0.000001, 0.0000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__max_iter': (10, 50),#, 100, 200, 300, 400, 500, 100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search started\n",
      "---------------------------------------\n",
      "Pipeline: ['vect', 'tfidf', 'clf']\n",
      "Grid Search Parameters:\n",
      "{'vect__max_df': (0.5, 1.0), 'vect__max_features': (None, 1000, 5000), 'vect__ngram_range': ((1, 1), (1, 2)), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__alpha': (0.1, 0.01, 0.001), 'clf__penalty': ('l2', 'elasticnet'), 'clf__max_iter': (10, 50)}\n",
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 24.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 45.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 72.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 88.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 106.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed: 118.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7131.297s\n",
      "----------------------------------------------\n",
      "Best Score: 0.766\n",
      "-------------------------------------------\n",
      "Best Parameters:\n",
      "\tclf__alpha: 0.001\n",
      "\tclf__max_iter: 10\n",
      "\tclf__penalty: 'l2'\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, refit=True)\n",
    "\n",
    "print(\"Grid Search started\\n---------------------------------------\")\n",
    "print(\"Pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"Grid Search Parameters:\")\n",
    "print(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(np.array(em['Subject']), np.array(em['Category']))\n",
    "print(\"done in %0.3fs\\n----------------------------------------------\" % (time() - t0))\n",
    "\n",
    "print(\"Best Score: %0.3f\\n-------------------------------------------\" % grid_search.best_score_)\n",
    "print(\"Best Parameters:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                'sgdclassifier__learning_rate':['constant','optimal','invscaling'],\n",
    "                'sgdclassifier__eta0':[0.0,0.01,0.1,0.3,0.5,0.7],\n",
    "                'sgdclassifier__alpha':[0.0001,0.001,0.01,0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__alpha', 'clf__average', 'clf__class_weight', 'clf__early_stopping', 'clf__epsilon', 'clf__eta0', 'clf__fit_intercept', 'clf__l1_ratio', 'clf__learning_rate', 'clf__loss', 'clf__max_iter', 'clf__n_iter_no_change', 'clf__n_jobs', 'clf__penalty', 'clf__power_t', 'clf__random_state', 'clf__shuffle', 'clf__tol', 'clf__validation_fraction', 'clf__verbose', 'clf__warm_start'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(grid_search, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    'hey there',\n",
    "    'Coorg trip advice',\n",
    "    'movie tickets for sale',\n",
    "    'Advice needed for treatment of hair fall',\n",
    "    'Moving out sale',\n",
    "    'RE: Selling Honda City'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['deal discrepancies', 'calendar', 'logistics', 'calendar',\n",
       "       'logistics', 'deal discrepancies'], dtype='<U19')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.predict(np.array(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    'hey there',\n",
    "    'Coorg trip advice',\n",
    "    'movie tickets for sale',\n",
    "    'Advice needed for treatment of hair fall',\n",
    "    'Moving out sale',\n",
    "    'RE: Selling Honda City'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['deal discrepancies', 'calendar', 'logistics', 'calendar',\n",
       "       'logistics', 'deal discrepancies'], dtype='<U19')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.predict(np.array(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
